Counterpart code from the paper 'How to obtain efficient GPU kernels: an illustration using FMM & FGT'

Abstract:
Computing on graphics processors is maybe one of the most important developments in computational science to happen in decades.  Not since the arrival of the Beowulf cluster, which combined open source software with commodity hardware to truly democratize high-performance computing, has the community been so electrified.  Like then, the opportunity comes with challenges.  The formulation of scientific algorithms to take advantage of the performance offered by the new architecture requires rethinking core methods. Here, we have tackled fast summation algorithms (fast multipole method and fast Gauss transform), and applied algorithmic redesign for attaining performance on {\gpu}s.  The progression of performance improvements attained illustrates the exercise of formulating algorithms for the massively parallel architecture of the GPU.  The end result has been GPU kernels that run at over 500 Gigaflops on one NVIDIA Tesla C1060 card, thereby reaching close to practical peak. We can confidently say that GPU computing is not just a vogue, it is truly an irresistible trend in high-performance computing.